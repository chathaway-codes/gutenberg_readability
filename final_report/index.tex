\documentclass[]{article}

%opening
\title{Automated Reading Comprehension Clustering}
\author{Charles Hathaway, David Hedin}

\usepackage[final]{pdfpages}
\usepackage{graphicx}
\usepackage[style=ieee,backend=bibtex]{biblatex}

\bibliography{biblo}

\begin{document}

\maketitle

\begin{abstract}

Determining the level of readability of documents, especially books, has lots of application in the domain of education.
It helps to quantify and group books which may be used at a particular reading level, therefore enhancing the classroom expierence for both instructors and teachers.
In this paper, we ran an experiment with a variety of books obtained from Project Gutenberg \cite{gutenberg2015} organized by the project maintainers into a 2 groups; books for children, and adult fiction.
To further enhance the analysis of this project, we also used our features to try and cluster books into clusters defined by the Fleschâ€“Kincaid readability metrics \cite{kincaid1975derivation}.

\end{abstract}

\section{Introduction}

There are several systems currently used to classify books based on reading comprehension level, for numerous applications ranging from selecting books for classrooms, to measuring an individuals literacy skills for both medical (autism, dyslexia, etc.) and educational purposes.
In this paper, we analyze the results of utilizing several existing features to classify works in addition to a variety of novel features we created.
The primary goal is to cluster books in groups representing the original designation of books; adult fiction and children fiction.

Given an input of 2292 books (320 children, 2002 adult, with 32 books overlapping) we achieved an F-score of 87.5\%; this is a significant improvement over the 62\% baseline \footnote{There was some in-class discussion which suggested our baseline would be (total number of adult books)/(total number of books), which would put the baseline at around 86\%. After testing this experimentally, and reasoning things out, we concluded the true baseline would be 62\% as our algorithm had no idea what the sizes of the clusters were, and a truly random distribution would but half in each cluster, with one cluster having a higher chance of being correct than the other}

\section{Previous Works}

Although not extensively, we did evaluate and learn from a number of previous works.
Most significantly, we borrowed features from work done by Feng at al. \cite{feng2009cognitively}.
Feng focused on the concept of documents being difficult to read due to "items fall[ing] out of memory before they can be semantically encoded".
With this in mind, most of the novel metrics they designed focus on the number of entities mentioned in a document.
Their work was motived to help organize documents by readability for adults with intellectual disabilities.

Our work is different than this in that we are using a much different data source (they were focused on news articles, where we are focused on fiction books).
In addition, instead of focusing on adults with intellectual disabilities directly, we are instead more interested in examining books in relation to each other, with the assumption that easier to read books would be easier for both adults with ID, and developmentally delayed children.

\section{Methodology and system design}

The system is written in Python, and utilizes the Natural Language ToolKit (NLTK) \cite{loper2002nltk}.
The toolkit was trained using a portion of the Penn Treebank and the Conll2000 corpus.
The potion used was provided by NLTK via it's distribution manager (nltk.download).

Once the corpus was downloaded and configured, we built the system in 3 stages.
The first was retrieving the document to be analyzed; this was done ahead of time to verify the availability of the resources, and allow us to develop without worrying about going over any kind of bandwidth quota.
However, now that development is mostly complete, this step can be skipped and documents can be downloaded directly from a Gutenberg mirror.

The next step is to process the data.
This includes using the trained POS tagger, chunker, and NE tagger on all documents, running a variety of feature function on the documents, then recording the device.
Our system writes data to a CSV file as it is processed, and uses a "Book" object to cache the results of each computation between features.

And lastly, we score the results of the system.
This process involves reading in the CSV file and training our clustering algorithmn, then clustering our test data.
Ultimately, we determine the accuracy of the clustering using a $B^3$ scoring system.

To get a better idea of how each feature contributes to the final score, we run the system multiple times with every combination of features.

\subsection{System Usage}

To setup your system, please follow the instruction in the README.md file provided in the repository.

Once configured, simply run "python src/main.py --help" to get a list of options and configuration settings.
The simplest invocation of this command requires you to specify 2 files; the list of adult books, and the list of children books.
It will then print the clustering results with the default feature-set to standard out.

\section{Linguistic Features}

\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| c |} \hline
			Average number of words per sentence \\ \hline
			Average number of syllables per word \\ \hline
			Percentage of words with more than 3 syllables \\ \hline
			Average number of noun phrases per sentence \\ \hline
			Average number of common and proper nouns per sentence \\ \hline
			Average number of verb phrases per sentence \\ \hline
			Average number of adjectives per sentence \\  \hline
			Average number of conjunctions per sentence \\ \hline
			Average number of prepositional phrases per sentence \\ \hline
			Total number of noun phrases in document \\ \hline
			Total number of common and proper nouns in document \\ \hline
			Total number of verb phrases in document \\ \hline
			Total number of adjectives in document \\ \hline
			Total number of conjunctions in document \\ \hline
			Total number of prepositional phrases in document \\ \hline
			Number of entity mentions in document \\ \hline
			Number of unique entities in document \\ \hline
			Average number of entity mentions per sentence \\ \hline
			Average number of unique entities per sentence \\ \hline
		\end{tabular}
	\end{center}
	\caption{List of possible features from previous work\cite{}}
	\label{table:features1}
\end{table}

\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| c |} \hline
			Average word length in document \\ \hline
			Total number of unique words in document\\ \hline
			Ratio of unique words to total number of words in document \\ \hline
			Ratio of proper nouns to common nouns in document \\ \hline
			Length of document \\ \hline
			Average number of proper nouns per sentence \\ \hline
			Total number of proper nouns in document \\ \hline
			Total number of passive sentences in document \\ \hline
			Average number of prepositional phrases per sentence \\ \hline
			Total number of prepositional phrases in document \\ \hline
			
		\end{tabular}
	\end{center}
	\caption{List of possible new features}
	\label{table:features2}
\end{table}

\section{Results}

Due to sheer number of results, we merely provided a synopsis consisting of the highest, middle, and worst combination of features.

\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| c | c | c | c |} \hline
			f-score & recall & precision & combination of filters \\
			\hline
			0.8748384028 & 0.8787276342 & 0.8709834469 & combination 1\\
			0.6690611348 & 0.7238314176 & 0.6219964193 & combination 2 \\
			0.6041246854 & 0.74973942 & 0.505873669 &	combination 3 \\
			0.622534193098 & 0.508873237259 & 0.801571724969 & Baseline \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Results of clustering}
	\label{table:features2}
\end{table}

Feature combinations:

\begin{enumerate}
	\item  average\_number\_of\_adjectives + average\_number\_of\_common\_proper\_nouns
	\item total\_number\_of\_adjectives+average\_verb\_phrases + total\_number\_of\_noun\_phrases + total\_number\_of\_prepositional\_phrases + total\_number\_of\_verb\_phrases + total\_number\_of\_conjunctions+average\_number\_of\_common\_proper\_nouns
	\item average\_verb\_phrases + ratio\_of\_common\_to\_proper\_nouns+average\_number\_of\_conjunctions + average\_prepositional\_phrases
\end{enumerate}

\section{Conclusion}


\printbibliography

\end{document}
